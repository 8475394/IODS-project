# Clustering and classification

&nbsp;

## Brief description of the dataset

&nbsp;

In this chapter the purpose is to get familiar with clustering and classification. I'm going to use a dataset called **Boston** which contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. The data was originally published by Harrison, D. and Rubinfeld, D.L. "Hedonic prices and the demand for clean air", J. Environ. Economics & Management, vol.5, 81-102, 1978. You can find some additional information [here](http://www.clemson.edu/economics/faculty/wilson/R-tutorial/analyzing_data.html) and [here](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/).

This dataset is actually already loaded into R thus let's take a look at some details.

&nbsp;

<p class="text-danger">**DATA VARIABLES**</p>

* crim - per capita crime rate by town
* zn - proportion of residential land zoned for lots over 25,000 sq.ft.
* indus - proportion of non-retail business acres per town
* chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 
* nox - nitrogen oxides concentration (parts per 10 million)
* rm - average number of rooms per dwelling
* age - proportion of owner-occupied units built prior to 1940
* dis - weighted mean of distances to five Boston employment centres
* rad - index of accessibility to radial highways
* tax - full-value property-tax rate per \$10,000
* ptratio - pupil-teacher ratio by town
* black - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
* lstat - lower status of the population (percent)
* medv - median value of owner-occupied homes in \$1000s


&nbsp;

<p class="text-danger">**STRUCTURE OFTHE BOSTON DATASET**</p>

From the structure table you can notice that there are 506 observation and 14 variables including 13 continuous variables and 1 binary-valued variable "chas". 

&nbsp;

```{r}
library(MASS)
data("Boston")
str(Boston)
```

&nbsp;

## Summaries and graphs

&nbsp;

In this section I'm going to explore the distributions of data variables even further. First I show you a summary table which contains the distributions of each variable where the variables are divided into quartiles including the minimum and maximum values. 

After this I'll show you the histograms of these variables excluding chas and black. Chas is a dummy variable concisting the values 1 or 0. The reason for me of dropping black is ethical. Remember that the current dataset is already "long in the tooth". However, from the histograms you can have a better understanding of the variation of each variable.

At the end of this chapter I'll explore the correlations between the variables and demonstrate them by means of the the correlation matrix table and the correlation matrix plot.  

&nbsp;

<p class="text-success">**SUMMARY TABLE OF DATA VARIABLES**</p>

From the summary table below you can perceive that all of the ditributions varies between very different scales.

&nbsp;


```{r}
summary(Boston)
```

&nbsp;

<p class="text-success">**HISTOGRAMS OF CRIM, ZN, INDUS AND NOX**</p>



```{r, fig.width=8, fig.height=8, message=FALSE, warning=FALSE}
library(ggplot2); library(gridExtra);library(dygraphs)

# INITIALIZING THE PLOTS

# Histogram of crim
hcrim <- qplot(Boston$crim, geom = "histogram", binwidth = 6, main = "Crime rate by town", xlab = "crime rate", col = I("grey"), fill = I("chartreuse3"))

# Histogram of zn
hzn <- qplot(Boston$zn, geom = "histogram", binwidth = 6, main = "Prop. of residential land zoned", xlab = "zn", col = I("grey"), fill = I("orange"))

# Histogram of indus
hindus <- qplot(Boston$indus, geom = "histogram", binwidth = 3, main = "Prop. of non-retail business", xlab = "indus", col = I("grey"), fill = I("mediumorchid"))

hnox <- qplot(Boston$nox, geom = "histogram", binwidth = 0.02, main = "Nitrogen oxides concentration", xlab = "nox", col = I("grey"), fill = I("blue"))


# Combining the plots using the function grid.arrange() [in gridExtra] 
  
# Additional information from (http://www.sthda.com/english/wiki/ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page-r-software-and-data-visualization)

grid.arrange(hcrim, hzn, hindus, hnox, ncol=2, nrow =2)
```

&nbsp;

<p class="text-success">**HISTOGRAMS OF RM, AGE, DIS AND RAD**</p>



```{r, fig.width=8, fig.height=8, message=FALSE, warning=FALSE}
library(ggplot2); library(gridExtra); library(dygraphs)

# INITIALIZING THE PLOTS

hrm <- qplot(Boston$rm, geom = "histogram", binwidth = 0.5, main = "Number of rooms", xlab = "rooms", col = I("grey"), fill = I("yellow"))

hage <- qplot(Boston$age, geom = "histogram", binwidth = 10, main = "Owner-occupied units", xlab = "age of the unit", col = I("grey"), fill = I("chocolate"))

hdis <- qplot(Boston$dis, geom = "histogram", binwidth = 1, main = "distances to 5 employment centres", xlab = "distance", col = I("grey"), fill = I("cornflowerblue"))

hrad <- qplot(Boston$rad, geom = "histogram", binwidth = 1, main = "Access to radial highways", xlab = "radial highways", col = I("grey"), fill = I("coral"))

# Combining the plots using the function grid.arrange() [in gridExtra] 

grid.arrange(hrm, hage, hdis, hrad, ncol=2, nrow =2)
```

&nbsp;

<p class="text-success">**HISTOGRAMS OF TAX, PTRATIO, LSTAT AND MEDV**</p>



```{r, fig.width=8, fig.height=8, message=FALSE, warning=FALSE}
library(ggplot2); library(gridExtra); library(dygraphs)

# INITIALIZING THE PLOTS

htax <- qplot(Boston$tax, geom = "histogram", binwidth = 30, main = "Property tax rate per $10,000", xlab = "tax rate", col = I("grey"), fill = I("red"))

hptratio <- qplot(Boston$ptratio, geom = "histogram", binwidth = 1, main = "Pupil-teacher ratio by town", xlab = "pupil-teacher ratio", col = I("grey"), fill = I("purple"))

hlstat <- qplot(Boston$lstat, geom = "histogram", binwidth = 2, main = "Lower status of the population %", xlab = "lower status", col = I("grey"), fill = I("palegreen4"))

hmedv <- qplot(Boston$medv, geom = "histogram", binwidth = 4, main = "Owner-occupied homes in $1000s", xlab = "Owner-occupied homes", col = I("grey"), fill = I("violetred2"))

# Combining the plots using the function grid.arrange() [in gridExtra] 

grid.arrange(htax, hptratio, hlstat, hmedv, ncol=2, nrow =2)
```

<br>
<br>

<p class="text-success">**CORRELATION MATRIX BETWEEN DATA VARIABLES**</p>



```{r, message=FALSE, warning=FALSE}
library(tidyverse)
cor_matrix <- cor(Boston) %>% round(digits = 2)
cor_matrix
```

&nbsp;

<p class="text-success">**PLOTTING THE CORRELATION MATRIX**</p>

I used the corrplot() to better demonstrate the above correlation matrix. You can see this plot below and it is good to know that positive correlations are displayed in blue and negative correlations in red color. Color intensity and the size of the circle are proportional to the correlation coefficients. The legend color shows the correlation coefficients and the corresponding colors. I also added the values of correlations of each pair even though the plot is not so beautiful after this.

You perceive that the highest correlations are between nox and indus as well as between tax and indus. This is kind of logical that nitrogen oxide levels and tax rates are higher in industrial areas.

Within the next chapters I'm going to concentrate more on crime rates as a target variable thus its also convenient to explore here the relationships between crime rate and other variables. You can see the highest positive correlations are between crime rate and rad as well as crime rate and tax rate. I'm not sure what these are actuallyt telling us. An by this I mean that sometimes variables can correlate without any specific cause - effect impacts. Here the crime rate by town correlates positive with accessibility to radial highways but I can't figure out any "real" reasons why this is happening. Maybe the criminals do their crimes in towns from where they can get easily away :D :D 

And maybe the positive correlation between crime rate and tax rate indicates that there are more crimes in those areas with bigger properties because bigger properties have bigger taxes. Also there might live wealthier people in bigger properties which may be more tempting for example to the thiefs. I'm not sure what kind of crimes the crime rate contains but I think you get my point. 

Then there are more moderate negative correlations between crime rate and medv as well as between crime rate and dis. The medv indicates the median value of owner-occupied homes in $1000s and the correlation suggests that the crime rate is higher when the median value of owner-occupied homes is lower and vice versa. I don't try to interpret this unattached from the bigger picture because there can be several reasons fro this. The dis indicates the weighted mean of distances to five Boston employment centres. Thus the negative correlation tells us that the higher the crime rate, the lower the mean distance and vice versa. This may be logical in the way that there can more crimes near these centres (shorter distance) comparing the more rural areas (greater distance).  

Note that under the corrplot you can find a pair (scatter plot) matrix which illustrates also the relationships between different variables.

&nbsp;

```{r, fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
library(corrplot); library(dygraphs)
corrplot(cor_matrix, method = "circle", main = "Correlation matrix of Boston variables", tl.cex = 1, addCoef.col = "black")

library(ggplot2)
library(GGally)
pairs(Boston [-4], main = "Scatter plot matrix")
```


&nbsp;

## Scaling the Boston dataset

&nbsp;

In this chapter I'm going to scale or standardize the whole dataset. 


&nbsp;


<p class="text-primary">**SUMMARIES OF THE SCALED DATA**</p>

As you can see from the table below, the distributions of the variables changed after stardardizing the dataset. Now the mean of every variable is zero and the observations varies on both sides of zero. This means that the different distributions are now more comparable for the further purpose.


&nbsp;

```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
```

&nbsp;


<p class="text-primary">**CATEGORIZING THE CRIME RATE**</p>

There are a lot of technicalities in this part thus I decided to describe the steps inside the r code chunks. the main point here is to create a new categorical variable based on the scaled crime rate and split it on to 4 categories by means of the its quantiles labelled as "low", "med_low", "med_high" and "high". After this I remove the "original" scaled crime rate from the dataset.


&nbsp;

```{r}
# Converting the scaled data into a data frame
boston_scaled <- as.data.frame(boston_scaled)

scaled_crim <- boston_scaled$crim

# Using the quantiles as the break points in the categorical variable of scaled_crim
bins <- quantile(scaled_crim)
bins

# Creating a categorical variable "crime"
boston_scaled$crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

crime <- boston_scaled$crime
table(crime)

# Verifying the content
colnames(boston_scaled)
class(boston_scaled)

```

&nbsp;

```{r}
# Dropping the old crime rate from the dataset
library(dplyr)
boston_scaled <- dplyr::select(boston_scaled, -crim)

# Veryfying the content
colnames(boston_scaled)
summary(boston_scaled)
```


&nbsp;

<p class="text-primary">**CREATING THE TRAIN AND TEST SETS**</p>

```{r}
# Choosing randomly 80 % of the row
n <- nrow(boston_scaled)
ind <- sample(n, size = n * 0.8)

# Creating train and test sets
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]

# Inspecting the content
str(train)
str(test)
colnames(train)
colnames(test)

```


&nbsp;

## Linear Discriminant Analysis, LDA

&nbsp;

Linear discriminant analysis is a classification technique and its preferred over Logistic regression if the target variable has more than two classes. LDA has some assumption about the data and one of these is that the input variables are normally distributed. It also assumes that each attribute has the same variance  [Reference](http://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/).

Note that the data was standardized earlier in terms of the latter assumption. 

&nbsp;

<p class="text-warning">**FITTING THE LINEAR DISCRIMINANT ANALYSIS**</p>

From the table below we can see for example that the model predicts 24.3% of crime rate in the training set correspond to the low crime rate. The output provides also the group means which are the average of each predictor within each class.

LDA defines as many "discriminant functions" as the number of categories of the outcome minus one. I have four categories thus the LDA output shows three functions under the headline "Proportion of trace". These functions are ordered by the amount of variance that they explain. 

&nbsp;

```{r, message=FALSE, warning=FALSE}
library(MASS)
lda.fit <- lda(crime~., data = train)
lda.fit
```


&nbsp;

```{r, fig.height=7, fig.width=10}
# Creating a numeric vector of the train sets crime classes for plotting purposes
classes <- as.numeric(train$crime)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# plotting the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1.5)

```



&nbsp;

## Predicting the classes with the Linear Discriminant Analysis, LDA 

&nbsp;

LDA makes predictions by estimating the probability that a new set of inputs belongs to each class. The class that gets the highest probability is the output class and a prediction is made  [Reference](http://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/).

First I'm going to save the four crime classes from the test set to "correct_classes" and then I remove the variable from test set, so that I can use the categorized crime as a target variable in LDA prediction when predicting the test data.  

&nbsp;

<p class="text-danger">**SAVING THE CRIME CATEGORIES**</p>


```{r, message=TRUE, warning=TRUE}
# Saving the crime classes from the test data into correct_classes
correct_classes <- test$crime

# Removing crime from the test set
library(dplyr)
test <- dplyr::select(test, -crime)

# Veryfying the content of test and correct_classes
str(test)
colnames(test)
str(correct_classes)
```


&nbsp;

```{r}
summary(test)
```

&nbsp;

<p class="text-danger">**PREDICTING CLASSES WITH TEST DATA**</p>

In this part I'm going to predict the classes with the test data. In addition to our exercises in DataCamp I used the actual command of confusionMatrix() to illustrate the results more clearly. And because of this the table of predicted values comes actually twice. I kept the first table because I wanted to be sure that I manged to code the confusion matrix correctly.

From the confusion matrix we can see that the estimated accuracy on the final model is 73.5%. You can see the same result also from the table where the correct classes are against the predicted classes. And if you like you can count the accuracy % by hand: sum of correctly predicted values divided with the total amount of values. 

&nbsp;


```{r, message=FALSE, warning=FALSE}
# Predicting classes with the test data
lda.pred <- predict(lda.fit, newdata = test)
lda.pred$class

# Cross tabulating the result
table1 <- table(correct = correct_classes, predicted = lda.pred$class)
table1

# Some of the packages can be irrelevant. Different sites proposed different packages and then R noted to require some other packages
library(caret); library(lattice); library(mlbench); library(e1071)
confusionMatrix(correct_classes, lda.pred$class)
```

&nbsp;

## Clustering

&nbsp;

Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a dataset. When we cluster the observations of a dataset, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other.

In this chapter I'm going to reload the Boston dataset for the clustering purposes. First I'm going to scale the variables to get comparable distances between the observations. After this I'm going to run the k-means algorithm and investigate the optimal number of clusters.

K-means clustering is an unsupervised learning algorithm which means that there is no outcome to be predicted but instead tries to find patterns in the data.



&nbsp;

<p class="text-success">**RELOADING THE BOSTON DATASET**</p>


```{r}
reboston <- Boston
str(reboston)
```

&nbsp;

<p class="text-success">**SUMMARY OF NOT SCALED BOSTON DATASET**</p>

```{r}
summary(reboston)
```

&nbsp;

<p class="text-success">**SUMMARY OF SCALED BOSTON DATASET**</p>

```{r}
reboston_scaled <- scale(reboston)
summary(reboston_scaled)
```

&nbsp;


<p class="text-success">**CALCULATING THE DISTANCES**</p>

Here I'm going to calculate the Euclidean distance which is a very common ordinary or straight-line distance measure. It is used by measuring similarity or dissimilarity of objects.

&nbsp;

```{r}
# Euclidean distance
dist_eu <- dist(reboston_scaled)
summary(dist_eu)

```

&nbsp;


<p class="text-success">**K-MEANS AND INITIAL AMOUNT OF CLUSTERS**</p>

K-means is one of the best-known clustering approaches whish is applied to partition the observations into a pre-specified number of clusters.

In this section I present some k-means pairplots adjusted with the euclidean distance and some random counts of centers. 

 

&nbsp;

```{r, fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
library(dygraphs)


km_eu <- kmeans(dist_eu, centers = 2)
pairs(reboston_scaled, col = km_eu$cluster, main = "Pairs with 2 clusters")


km_eu2 <- kmeans(dist_eu, centers = 4)
pairs(reboston_scaled, col = km_eu2$cluster, main = "Pairs with 4 clusters")


```



&nbsp;


<p class="text-success">**DETERMINING THE NUMBER OF CLUSTERS**</p>


Here the purpose is to find the optimal number of clusters. Idea behind the K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. There are different methods to achieve this goal and I'm going to use total within sum of squares, WCSS, which can be calculated with a use of formula below:

$WCSS = \sum_i^N (x_i - centroid)^2$

&nbsp;


The formula below is adopted from the DataCamp exercises as such. The task is to found an appropriate amount of clusters. The DC guidelines adviced to choose that number of clusters where the total WCSS drops radically. So if you look at the figure below, the most radical drop happens after the first number suggestings that the optimal amount could be 2 clusters. 

On the other hand the statistical textbooks (e.g. http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf) suggest that the clustering "results" may be optimal when the within-cluster variation is as small as possible. 

I thought I could somehow combine the above advices and this is what I concluded: The second "cluster point" doens't reflect the smallest variation even though there is a radical change after first "point. It seems that there are some drops between the second and sixth "point", but between the fifth and sixth the within-cluster variation doesn't seem be to be so different anymore. After this you may already guess that I'm suggesting 5 clusters. But I wouldn't say that this has to be a final decision without using other tools and methods.    


&nbsp;

```{r}
set.seed(123) # keeping the results same
k_max <- 10 # maximum number of clusters
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
plot(1:k_max, twcss, type='b', main = "Number of proposed clusters")
```

&nbsp;

<p class="text-success">**USING THE ELBOW METHOD TO DETERMINE THE NUMBER OF CLUSTERS**</p>

Then there is a so called Elbow Method which also uses the within-cluster variance. You can adopt the method and algorith [here](http://www.sthda.com/english/wiki/determining-the-optimal-number-of-clusters-3-must-known-methods-unsupervised-machine-learning). In this site the author suggest that "the location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters".

Below you can perceive two plots with the Elbow method. I have to say that I'm not sure anymore what could be the "correct" number of clusters. Here I would rather say 2 than 5. Note that I draw the dashed lines just to compare the results. 

But I'm not happy yet with the results. So let's take a look at one more method after this.


&nbsp;

```{r}
set.seed(123)

# Computing and plotting wss for k = 2 to k = 10
k.max <- 10 # Maximal number of clusters
data <- reboston_scaled
wss <- sapply(1:k.max, 
        function(k){kmeans(data, k, nstart=10 )$tot.withinss})
plot(1:k.max, wss,
       type="b", pch = 1, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares", main = "The amount of clusters")
abline(v = 2, lty =2)

# Another functions, same method
library(factoextra); require(ggplot2)
fviz_nbclust(reboston_scaled, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2)


```


&nbsp;

<p class="text-success">**USING THE "ALL VALIDATION INDICES" TO DETERMINE THE NUMBER OF CLUSTERS**</p>

This method is also adopted from the website I introduced above. This might be "heavy" method when it computes all the 30 indices for determining the optimal number of clusters (see ?NbClust for additional information). Note that the command "all" actually drops some indices for the sake of faster computing. 


**So according to this method of majority rule, the best number of clusters is 2.**


&nbsp;

```{r}
library(NbClust)

nb <- NbClust(reboston_scaled, distance = "euclidean", min.nc = 2,
        max.nc = 10, method = "complete", index ="all")
nb

# Visualizing the results
fviz_nbclust(nb) + theme_minimal()
```


&nbsp;

Here you can look again the pair matrix of two clusters.

&nbsp;

```{r, fig.height=10, fig.width=10, message=FALSE, warning=FALSE}

# Lets plot the pairs with two centers again

library(dygraphs)

pairs(reboston_scaled, col = km_eu$cluster, main = "Pairs with 2 clusters")
```


&nbsp;






