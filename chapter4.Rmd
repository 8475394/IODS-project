# Clustering and classification

&nbsp;

## Brief description of the dataset

&nbsp;

In this chapter the purpose is to get familiar with clustering and classification. I'm going to use a dataset called **Boston** which contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. The data was originally published by Harrison, D. and Rubinfeld, D.L. "Hedonic prices and the demand for clean air", J. Environ. Economics & Management, vol.5, 81-102, 1978. You can find some additional information [here](http://www.clemson.edu/economics/faculty/wilson/R-tutorial/analyzing_data.html) and [here](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/).

This dataset is actually already loaded into R thus let's take a look at some details.

&nbsp;

<p class="text-danger">**DATA VARIABLES**</p>

* crim - per capita crime rate by town
* zn - proportion of residential land zoned for lots over 25,000 sq.ft.
* indus - proportion of non-retail business acres per town
* chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 
* nox - nitrogen oxides concentration (parts per 10 million)
* rm - average number of rooms per dwelling
* age - proportion of owner-occupied units built prior to 1940
* dis - weighted mean of distances to five Boston employment centres
* rad - index of accessibility to radial highways
* tax - full-value property-tax rate per \$10,000
* ptratio - pupil-teacher ratio by town
* black - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
* lstat - lower status of the population (percent)
* medv - median value of owner-occupied homes in \$1000s


&nbsp;

<p class="text-danger">**STRUCTURE OFTHE BOSTON DATASET**</p>

From the structure table you can notice that there are 506 observation and 14 variables including 13 continuous variables and 1 binary-valued variable "chas". 

&nbsp;

```{r}
library(MASS)
data("Boston")
str(Boston)
```

&nbsp;

## Summaries and graphs

&nbsp;

In this section I'm going to explore the distributions of data variables even further. First I show you a summary table which contains the distributions of each variable where the variables are divided into quartiles including the minimum and maximum values. 

After this I'll show you the histograms of these variables excluding chas and black. Chas is a dummy variable concisting the values 1 or 0. The reason for me of dropping black is ethical. Remember that the current dataset is already "long in the tooth". However, from the histograms you can have a better understanding of the variation of each variable.

At the end of this chapter I'll explore the correlations between the variables and demonstrate them by means of the the correlation matrix table and the correlation matrix plot.  

&nbsp;

<p class="text-success">**SUMMARY TABLE OF DATA VARIABLES**</p>

From the summary table below you can perceive that all of the ditributions varies between very different scales.

&nbsp;


```{r}
summary(Boston)
```

&nbsp;

<p class="text-success">**HISTOGRAMS OF CRIM, ZN, INDUS AND NOX**</p>


&nbsp;

```{r, fig.width=8, fig.height=8, message=FALSE, warning=FALSE}
library(ggplot2); library(gridExtra);library(dygraphs)

# INITIALIZING THE PLOTS

# Histogram of crim
hcrim <- qplot(Boston$crim, geom = "histogram", binwidth = 6, main = "Crime rate by town", xlab = "crime rate", col = I("grey"), fill = I("chartreuse3"))

# Histogram of zn
hzn <- qplot(Boston$zn, geom = "histogram", binwidth = 6, main = "Prop. of residential land zoned", xlab = "zn", col = I("grey"), fill = I("orange"))

# Histogram of indus
hindus <- qplot(Boston$indus, geom = "histogram", binwidth = 3, main = "Prop. of non-retail business", xlab = "indus", col = I("grey"), fill = I("mediumorchid"))

hnox <- qplot(Boston$nox, geom = "histogram", binwidth = 0.02, main = "Nitrogen oxides concentration", xlab = "nox", col = I("grey"), fill = I("blue"))


# Combining the plots using the function grid.arrange() [in gridExtra] 
  
# Additional information from (http://www.sthda.com/english/wiki/ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page-r-software-and-data-visualization)

grid.arrange(hcrim, hzn, hindus, hnox, ncol=2, nrow =2)
```

&nbsp;

<p class="text-success">**HISTOGRAMS OF RM, AGE, DIS AND RAD**</p>


&nbsp;

```{r, fig.width=8, fig.height=8, message=FALSE, warning=FALSE}
library(ggplot2); library(gridExtra); library(dygraphs)

# INITIALIZING THE PLOTS

hrm <- qplot(Boston$rm, geom = "histogram", binwidth = 1, main = "Number of rooms", xlab = "rooms", col = I("grey"), fill = I("yellow"))

hage <- qplot(Boston$age, geom = "histogram", binwidth = 10, main = "Owner-occupied units", xlab = "age of the unit", col = I("grey"), fill = I("chocolate"))

hdis <- qplot(Boston$dis, geom = "histogram", binwidth = 1, main = "distances to 5 employment centres", xlab = "distance", col = I("grey"), fill = I("cornflowerblue"))

hrad <- qplot(Boston$rad, geom = "histogram", binwidth = 1, main = "Access to radial highways", xlab = "radial highways", col = I("grey"), fill = I("coral"))

# Combining the plots using the function grid.arrange() [in gridExtra] 

grid.arrange(hrm, hage, hdis, hrad, ncol=2, nrow =2)
```

&nbsp;

<p class="text-success">**HISTOGRAMS OF TAX, PTRATIO, LSTAT AND MEDV**</p>


&nbsp;

```{r, fig.width=8, fig.height=8, message=FALSE, warning=FALSE}
library(ggplot2); library(gridExtra); library(dygraphs)

# INITIALIZING THE PLOTS

htax <- qplot(Boston$tax, geom = "histogram", binwidth = 30, main = "Property tax rate per $10,000", xlab = "tax rate", col = I("grey"), fill = I("red"))

hptratio <- qplot(Boston$ptratio, geom = "histogram", binwidth = 1, main = "Pupil-teacher ratio by town", xlab = "pupil-teacher ratio", col = I("grey"), fill = I("purple"))

hlstat <- qplot(Boston$lstat, geom = "histogram", binwidth = 2, main = "Lower status of the population %", xlab = "lower status", col = I("grey"), fill = I("palegreen4"))

hmedv <- qplot(Boston$medv, geom = "histogram", binwidth = 4, main = "Owner-occupied homes in $1000s", xlab = "Owner-occupied homes", col = I("grey"), fill = I("violetred2"))

# Combining the plots using the function grid.arrange() [in gridExtra] 

grid.arrange(htax, hptratio, hlstat, hmedv, ncol=2, nrow =2)
```

&nbsp;

<p class="text-success">**CORRELATION MATRIX BETWEEN DATA VARIABLES**</p>

&nbsp;

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
cor_matrix <- cor(Boston) %>% round(digits = 2)
cor_matrix
```

&nbsp;

<p class="text-success">**PLOTTING THE CORRELATION MATRIX**</p>




Positive correlations are displayed in blue and negative correlations in red color. Color intensity and the size of the circle are proportional to the correlation coefficients. The legend color shows the correlation coefficients and the corresponding colors.

&nbsp;

```{r, fig.width=10, fig.height=10}
library(corrplot); library(dygraphs)
corrplot(cor_matrix, method = "circle", main = "Correlation matrix of Boston variables", tl.cex = 1, addCoef.col = "black")

```


&nbsp;

## Scaling the Boston dataset

&nbsp;

In this chapter I'm going to scale or standardize the whole dataset. 


&nbsp;


<p class="text-primary">**SUMMARIES OF THE SCALED DATA**</p>

As you can see from the table below, the distributions of the variables changed after stardardizing the dataset. Now the mean of every variable is zero and the observations varies on both sides of zero. This means that the different distributions are now more comparable for the further purpose.


&nbsp;

```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
```

&nbsp;


<p class="text-primary">**CATEGORIZING THE CRIME RATE AND DIVIDING THE DATASET TO TRAIN AND TEST SETS**</p>

There are a lot of technicalities in this part thus I decided to describe the steps inside the r code chunks. the main point here is to create a new categorical variable based on the scaled crime rate and split it on to 4 categories by means of the its quantiles labelled as "low", "med_low", "med_high" and "high". After this I remove the "original" scaled crime rate from the dataset.


&nbsp;

```{r}
# Converting the scaled data into a data frame
boston_scaled <- as.data.frame(boston_scaled)

scaled_crim <- boston_scaled$crim

# Using the quantiles as the break points in the categorical variable of scaled_crim
bins <- quantile(scaled_crim)
bins

# Creating a categorical variable "crime"
boston_scaled$crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

crime <- boston_scaled$crime
table(crime)

# Verifying the content
colnames(boston_scaled)
class(boston_scaled)

```

&nbsp;

```{r}
# Dropping the old crime rate from the dataset
library(dplyr)
boston_scaled <- dplyr::select(boston_scaled, -crim)

# Veryfying the content
colnames(boston_scaled)
summary(boston_scaled)
```


&nbsp;

<p class="text-primary">**CREATING THE TRAIN AND TEST SETS**</p>

```{r}
# Choosing randomly 80 % of the row
n <- nrow(boston_scaled)
ind <- sample(n, size = n * 0.8)

# Creating train and test sets
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]

# Inspecting the content
str(train)
str(test)
colnames(train)
colnames(test)

# Saving crime classes from the test dat into correct_classes
correct_classes <- test$crime

# Removing crime from the test set
library(dplyr)
test <- dplyr::select(test, -crime)

# Veryfying the content of test and correct_classes
str(test)
colnames(test)
str(correct_classes)

```


&nbsp;

