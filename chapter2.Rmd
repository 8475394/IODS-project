# Regression and model validation

In this section I'm going to analyze the data set of students who participated to the course called **Introduction to Social Statistics** at Helsinki University in the fall 2014. These 183 students were asked to participate to the survey about their learning approaches consisting deep, strategic and surface orientated learning. In addition the students were asked to evaluate their attitude towars statistics.

You can find some basic information about the variables [here](http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt)  


*Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using  


## Introducing the data set

I'm going to use partly the above mentioned data which still contains the different learning orientations and attitudes towars statistics. In addition the data variables consist students' age, gender and course exam points.  


**[Here]("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/learning2014.txt) you can find the data set what I'm going to use in this analysis**

```{r}
students2014 <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/learning2014.txt", header = TRUE, sep = ",")
```


**Next I'm going to explore what kind of material the students2014 data set contains**

As you can see below, there is 166 observations i.e. students and 7 variables. The variables contains the age and gender of the students. The genders can be recogniced **F** as woman and **M** as man. 

The attitudes and learning orientations were measured on the [Likert scale](https://en.wikipedia.org/wiki/Likert_scale) from 1 to 5. The attitude towards statistics is a sum of 10 questions, the deep orientation is 12, the strategic is 8 and the surface orientation is a sum of 12 question. Last but not least the data contains the exam points of the course.

Under the structure table you can see there is no missing data (False = 1162). This is useful information considering the regression modeling. 



```{r}
str(students2014)
table(is.na(students2014))
```



## Summaries and graphs 

Underneath you can see the data summaries and histograms of the variables (excluding gender). The data contains 110 (66 %) women and 56 (34 %) men. The students' age range between 17 and 55 where the mean age is around 25 and median is 22. The histogram of age shows that there is some skewness in the distribution. 

As mentioned earlier the attitudes towards statistics and the learning orientations were measured on the Likert scale from 1 to 5. You can see that especially the deep learning orientation seems to be quite high among students whereas other learning orientations seems to distribute somewhat normally which is the case also with attitude. 

The exam points seems to vary to some extent. THe points range between 7 and 33 where the mean points are around 23. 

```{r}
summary(students2014)
```

```{r}
library(ggplot2)
qplot(students2014$age, geom = "histogram", binwidth = 1, main = "Histogram for students' age", xlab = "age", col = I("grey"), fill = I("chartreuse3"))
```



```{r}
qplot(students2014$attitude, geom = "histogram", binwidth = 0.3, main = "Histogram for attitudes towards statistics", xlab = "attitude", col = I("grey"), fill = I("orange"))
```

```{r}
qplot(students2014$deep, geom = "histogram", binwidth = 0.3, main = "Histogram for deep learning orientation", xlab = "deep", col = I("grey"), fill = I("mediumorchid"))
```

```{r}
qplot(students2014$stra, geom = "histogram", binwidth = 0.3, main = "Histogram for strategic learning orientation", xlab = "strategic", col = I("grey"), fill = I("blue"))
```


```{r}
qplot(students2014$surf, geom = "histogram", binwidth = 0.3, main = "Histogram for surface learning orientation", xlab = "surface", col = I("grey"), fill = I("coral"))
```


```{r}
qplot(students2014$points, geom = "histogram", binwidth = 1, main = "Histogram for exam points", xlab = "Exam points", col = I("grey"), fill = I("yellow"))
```



**Next I'm going to show the scatter plots of our data variables to see if there could be any relationships between them.**

Later on I'm going to formulate the regression model to explain the exam points so we can concentrate at this point also on the relationships between other variables and points. 

First you can see the scatter plot matrix with every pair in the data (excluding gender). It seems that there could be some relationship between the attitude towards statistics and exam points. Because of this I printed a scatter plot where you can take a closer look for these variables coloured with gender.

In the pair matrix it seems that there could be some relationship also between strategic learning and exam points. But when you take a closer look to the separate scatter plot the dots seems to vary more. This is why I think you can "see more" from the bigger picture.

The last matrix you can see a set of scatter plots and distributions including the correlations between different variables. The matrix shows for example that the correlation between attitude and exam points is .437 whereas the correlation between strategic learning and points is only .146.


```{r}
library(ggplot2)
library(GGally)
pairs(students2014[-1], col = students2014$gender, main = "Scatter plot matrix")
```



```{r}
attitude <- students2014$attitude
points <- students2014$points
gender <- students2014$gender
ggplot(students2014, aes(x = attitude, y = points, col = gender)) + geom_point() + xlab("Attitudes towards statistics") + ylab("Exam points") + ggtitle("Attitudes vs. Exam points") + geom_smooth(method = "lm")
```


```{r}
stra <- students2014$stra
ggplot(students2014, aes(x = stra, y = points, col = gender)) + geom_point() + xlab("Strategic learning orientation") + ylab("Exam points") + ggtitle("Strategic learning vs. Exam points") + geom_smooth(method = "lm")
```


**Matrix of scatter plots and distributions including the correlations**

```{r}
ggpairs(students2014, mapping = aes(col = gender), lower = list(combo = wrap("facethist", bins = 20)))
```


## Multiple regression model

Now I'm going to choose three variables to explain the exam points and to adjust linear regression model. I'm choosing the variables at this point based on the highest correlation values with respect to exam points. Explanatory variables will be attitude (r = .437), strategic learning (r = .146) and surface approach (r = -.144).


**MODEL 1**

```{r}
Regression_model_1 <- lm(points ~ attitude + stra + surf, data = students2014)
summary(Regression_model_1)
```

As you can immediately see the regression model doesn't fit to the data because of the high p values with our explanatory variables strategic learning (p > .05) and surface learning (p > .05). This means I have to exclude these variables and either choose other variables from the data set or run the model only with attitude since it's the only significance variable at this point (p < .001). 

I am too curious to see other variables to explain the exam points even if the correlation matrix suggests there might not be any significant variable left to explain the exam points.

Beneath you can see that my hunch was correct because of the high p values with age and deep. In both cases p > .05. It seems I have to run one more model with attitude as an only explanatory variable.


**MODEL 2**

```{r}
Regression_model_2 <- lm(points ~ attitude + age + deep, data = students2014)
summary(Regression_model_2)
```

**MODEL 3**

```{r}
Regression_model_3 <- lm(points ~ attitude, data = students2014)
summary(Regression_model_3)
```

This is better :)

Our overall model seems to be significant: F(1,164) = 38.61, p < .001. The attitude seems to be significant as a predictor or as an explanatory variable: ?? = 3.5, p < .001. We can also note that the attitude explains around 19 % from the variation of the exam points. We can interpret this with multiple R-squared coefficient instead of adjusted coefficient because we have only one predictor in our model.  

